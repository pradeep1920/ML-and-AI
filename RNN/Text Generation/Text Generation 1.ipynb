{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "3nFzllJsJsEe",
    "outputId": "aeb5c011-0e68-43a0-d6b4-713116c75f8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%matplotlinb` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys\n",
    "\n",
    "%matplotlinb inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nwl01AEWJsEj",
    "outputId": "d518e5cc-e06c-4d94-d946-84145f9841fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"alice's adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. d\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "5JU2QTVnJsEl",
    "outputId": "c08ca956-6846-4e47-f290-b7385ee33d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars:\n",
      " ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '3', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "int_to_char:\n",
      " {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: '*', 8: ',', 9: '-', 10: '.', 11: '0', 12: '3', 13: ':', 14: ';', 15: '?', 16: '[', 17: ']', 18: '_', 19: 'a', 20: 'b', 21: 'c', 22: 'd', 23: 'e', 24: 'f', 25: 'g', 26: 'h', 27: 'i', 28: 'j', 29: 'k', 30: 'l', 31: 'm', 32: 'n', 33: 'o', 34: 'p', 35: 'q', 36: 'r', 37: 's', 38: 't', 39: 'u', 40: 'v', 41: 'w', 42: 'x', 43: 'y', 44: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(\"Chars:\\n\",chars)\n",
    "print(\"int_to_char:\\n\",int_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "XEeN_0Y7JsEn",
    "outputId": "c0bce15c-f1ec-4e3e-d14e-342341ab6ea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144409\n",
      "Total Vocab:  45\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "EI37jXnqJsEp",
    "outputId": "42fb33f4-8112-4e58-b0cb-1b5008ff2121"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144309\n",
      "dataX[:5]\n",
      " [[19, 30, 27, 21, 23, 4, 37, 1, 19, 22, 40, 23, 32, 38, 39, 36, 23, 37, 1, 27, 32, 1, 41, 33, 32, 22, 23, 36, 30, 19, 32, 22, 0, 0, 30, 23, 41, 27, 37, 1, 21, 19, 36, 36, 33, 30, 30, 0, 0, 38, 26, 23, 1, 31, 27, 30, 30, 23, 32, 32, 27, 39, 31, 1, 24, 39, 30, 21, 36, 39, 31, 1, 23, 22, 27, 38, 27, 33, 32, 1, 12, 10, 11, 0, 0, 0, 0, 0, 21, 26, 19, 34, 38, 23, 36, 1, 27, 10, 1, 22], [30, 27, 21, 23, 4, 37, 1, 19, 22, 40, 23, 32, 38, 39, 36, 23, 37, 1, 27, 32, 1, 41, 33, 32, 22, 23, 36, 30, 19, 32, 22, 0, 0, 30, 23, 41, 27, 37, 1, 21, 19, 36, 36, 33, 30, 30, 0, 0, 38, 26, 23, 1, 31, 27, 30, 30, 23, 32, 32, 27, 39, 31, 1, 24, 39, 30, 21, 36, 39, 31, 1, 23, 22, 27, 38, 27, 33, 32, 1, 12, 10, 11, 0, 0, 0, 0, 0, 21, 26, 19, 34, 38, 23, 36, 1, 27, 10, 1, 22, 33], [27, 21, 23, 4, 37, 1, 19, 22, 40, 23, 32, 38, 39, 36, 23, 37, 1, 27, 32, 1, 41, 33, 32, 22, 23, 36, 30, 19, 32, 22, 0, 0, 30, 23, 41, 27, 37, 1, 21, 19, 36, 36, 33, 30, 30, 0, 0, 38, 26, 23, 1, 31, 27, 30, 30, 23, 32, 32, 27, 39, 31, 1, 24, 39, 30, 21, 36, 39, 31, 1, 23, 22, 27, 38, 27, 33, 32, 1, 12, 10, 11, 0, 0, 0, 0, 0, 21, 26, 19, 34, 38, 23, 36, 1, 27, 10, 1, 22, 33, 41], [21, 23, 4, 37, 1, 19, 22, 40, 23, 32, 38, 39, 36, 23, 37, 1, 27, 32, 1, 41, 33, 32, 22, 23, 36, 30, 19, 32, 22, 0, 0, 30, 23, 41, 27, 37, 1, 21, 19, 36, 36, 33, 30, 30, 0, 0, 38, 26, 23, 1, 31, 27, 30, 30, 23, 32, 32, 27, 39, 31, 1, 24, 39, 30, 21, 36, 39, 31, 1, 23, 22, 27, 38, 27, 33, 32, 1, 12, 10, 11, 0, 0, 0, 0, 0, 21, 26, 19, 34, 38, 23, 36, 1, 27, 10, 1, 22, 33, 41, 32], [23, 4, 37, 1, 19, 22, 40, 23, 32, 38, 39, 36, 23, 37, 1, 27, 32, 1, 41, 33, 32, 22, 23, 36, 30, 19, 32, 22, 0, 0, 30, 23, 41, 27, 37, 1, 21, 19, 36, 36, 33, 30, 30, 0, 0, 38, 26, 23, 1, 31, 27, 30, 30, 23, 32, 32, 27, 39, 31, 1, 24, 39, 30, 21, 36, 39, 31, 1, 23, 22, 27, 38, 27, 33, 32, 1, 12, 10, 11, 0, 0, 0, 0, 0, 21, 26, 19, 34, 38, 23, 36, 1, 27, 10, 1, 22, 33, 41, 32, 1]]\n",
      "dataY[:5]\n",
      " [33, 41, 32, 1, 38]\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "print(\"dataX[:5]\\n\", dataX[:5])\n",
    "print(\"dataY[:5]\\n\", dataY[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mF6gCAyUJsEt",
    "outputId": "146e55f0-76a6-438a-cee8-d75af8e199ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[:2]:\n",
      " [[[0.42222222]\n",
      "  [0.66666667]\n",
      "  [0.6       ]\n",
      "  [0.46666667]\n",
      "  [0.51111111]\n",
      "  [0.08888889]\n",
      "  [0.82222222]\n",
      "  [0.02222222]\n",
      "  [0.42222222]\n",
      "  [0.48888889]\n",
      "  [0.88888889]\n",
      "  [0.51111111]\n",
      "  [0.71111111]\n",
      "  [0.84444444]\n",
      "  [0.86666667]\n",
      "  [0.8       ]\n",
      "  [0.51111111]\n",
      "  [0.82222222]\n",
      "  [0.02222222]\n",
      "  [0.6       ]\n",
      "  [0.71111111]\n",
      "  [0.02222222]\n",
      "  [0.91111111]\n",
      "  [0.73333333]\n",
      "  [0.71111111]\n",
      "  [0.48888889]\n",
      "  [0.51111111]\n",
      "  [0.8       ]\n",
      "  [0.66666667]\n",
      "  [0.42222222]\n",
      "  [0.71111111]\n",
      "  [0.48888889]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.66666667]\n",
      "  [0.51111111]\n",
      "  [0.91111111]\n",
      "  [0.6       ]\n",
      "  [0.82222222]\n",
      "  [0.02222222]\n",
      "  [0.46666667]\n",
      "  [0.42222222]\n",
      "  [0.8       ]\n",
      "  [0.8       ]\n",
      "  [0.73333333]\n",
      "  [0.66666667]\n",
      "  [0.66666667]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.84444444]\n",
      "  [0.57777778]\n",
      "  [0.51111111]\n",
      "  [0.02222222]\n",
      "  [0.68888889]\n",
      "  [0.6       ]\n",
      "  [0.66666667]\n",
      "  [0.66666667]\n",
      "  [0.51111111]\n",
      "  [0.71111111]\n",
      "  [0.71111111]\n",
      "  [0.6       ]\n",
      "  [0.86666667]\n",
      "  [0.68888889]\n",
      "  [0.02222222]\n",
      "  [0.53333333]\n",
      "  [0.86666667]\n",
      "  [0.66666667]\n",
      "  [0.46666667]\n",
      "  [0.8       ]\n",
      "  [0.86666667]\n",
      "  [0.68888889]\n",
      "  [0.02222222]\n",
      "  [0.51111111]\n",
      "  [0.48888889]\n",
      "  [0.6       ]\n",
      "  [0.84444444]\n",
      "  [0.6       ]\n",
      "  [0.73333333]\n",
      "  [0.71111111]\n",
      "  [0.02222222]\n",
      "  [0.26666667]\n",
      "  [0.22222222]\n",
      "  [0.24444444]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.46666667]\n",
      "  [0.57777778]\n",
      "  [0.42222222]\n",
      "  [0.75555556]\n",
      "  [0.84444444]\n",
      "  [0.51111111]\n",
      "  [0.8       ]\n",
      "  [0.02222222]\n",
      "  [0.6       ]\n",
      "  [0.22222222]\n",
      "  [0.02222222]\n",
      "  [0.48888889]]\n",
      "\n",
      " [[0.66666667]\n",
      "  [0.6       ]\n",
      "  [0.46666667]\n",
      "  [0.51111111]\n",
      "  [0.08888889]\n",
      "  [0.82222222]\n",
      "  [0.02222222]\n",
      "  [0.42222222]\n",
      "  [0.48888889]\n",
      "  [0.88888889]\n",
      "  [0.51111111]\n",
      "  [0.71111111]\n",
      "  [0.84444444]\n",
      "  [0.86666667]\n",
      "  [0.8       ]\n",
      "  [0.51111111]\n",
      "  [0.82222222]\n",
      "  [0.02222222]\n",
      "  [0.6       ]\n",
      "  [0.71111111]\n",
      "  [0.02222222]\n",
      "  [0.91111111]\n",
      "  [0.73333333]\n",
      "  [0.71111111]\n",
      "  [0.48888889]\n",
      "  [0.51111111]\n",
      "  [0.8       ]\n",
      "  [0.66666667]\n",
      "  [0.42222222]\n",
      "  [0.71111111]\n",
      "  [0.48888889]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.66666667]\n",
      "  [0.51111111]\n",
      "  [0.91111111]\n",
      "  [0.6       ]\n",
      "  [0.82222222]\n",
      "  [0.02222222]\n",
      "  [0.46666667]\n",
      "  [0.42222222]\n",
      "  [0.8       ]\n",
      "  [0.8       ]\n",
      "  [0.73333333]\n",
      "  [0.66666667]\n",
      "  [0.66666667]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.84444444]\n",
      "  [0.57777778]\n",
      "  [0.51111111]\n",
      "  [0.02222222]\n",
      "  [0.68888889]\n",
      "  [0.6       ]\n",
      "  [0.66666667]\n",
      "  [0.66666667]\n",
      "  [0.51111111]\n",
      "  [0.71111111]\n",
      "  [0.71111111]\n",
      "  [0.6       ]\n",
      "  [0.86666667]\n",
      "  [0.68888889]\n",
      "  [0.02222222]\n",
      "  [0.53333333]\n",
      "  [0.86666667]\n",
      "  [0.66666667]\n",
      "  [0.46666667]\n",
      "  [0.8       ]\n",
      "  [0.86666667]\n",
      "  [0.68888889]\n",
      "  [0.02222222]\n",
      "  [0.51111111]\n",
      "  [0.48888889]\n",
      "  [0.6       ]\n",
      "  [0.84444444]\n",
      "  [0.6       ]\n",
      "  [0.73333333]\n",
      "  [0.71111111]\n",
      "  [0.02222222]\n",
      "  [0.26666667]\n",
      "  [0.22222222]\n",
      "  [0.24444444]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.46666667]\n",
      "  [0.57777778]\n",
      "  [0.42222222]\n",
      "  [0.75555556]\n",
      "  [0.84444444]\n",
      "  [0.51111111]\n",
      "  [0.8       ]\n",
      "  [0.02222222]\n",
      "  [0.6       ]\n",
      "  [0.22222222]\n",
      "  [0.02222222]\n",
      "  [0.48888889]\n",
      "  [0.73333333]]]\n",
      "y[:2]:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "print(\"X[:2]:\\n\", X[:2])\n",
    "print(\"y[:2]:\\n\", y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YOXmywH7JsEw",
    "outputId": "17900ed4-5aff-4c5e-b418-d4ff6906b730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "144309/144309 [==============================] - 85s 592us/step - loss: 3.0871\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.08709, saving model to weights-improvement-01-3.0871-bigger.hdf5\n",
      "Epoch 2/50\n",
      "144309/144309 [==============================] - 84s 585us/step - loss: 2.9522\n",
      "\n",
      "Epoch 00002: loss improved from 3.08709 to 2.95223, saving model to weights-improvement-02-2.9522-bigger.hdf5\n",
      "Epoch 3/50\n",
      "144309/144309 [==============================] - 84s 584us/step - loss: 2.7942\n",
      "\n",
      "Epoch 00003: loss improved from 2.95223 to 2.79418, saving model to weights-improvement-03-2.7942-bigger.hdf5\n",
      "Epoch 4/50\n",
      "144309/144309 [==============================] - 85s 589us/step - loss: 2.7117\n",
      "\n",
      "Epoch 00004: loss improved from 2.79418 to 2.71167, saving model to weights-improvement-04-2.7117-bigger.hdf5\n",
      "Epoch 5/50\n",
      "144309/144309 [==============================] - 85s 588us/step - loss: 2.6481\n",
      "\n",
      "Epoch 00005: loss improved from 2.71167 to 2.64814, saving model to weights-improvement-05-2.6481-bigger.hdf5\n",
      "Epoch 6/50\n",
      "144309/144309 [==============================] - 84s 579us/step - loss: 2.5958\n",
      "\n",
      "Epoch 00006: loss improved from 2.64814 to 2.59576, saving model to weights-improvement-06-2.5958-bigger.hdf5\n",
      "Epoch 7/50\n",
      "144309/144309 [==============================] - 83s 578us/step - loss: 2.5504\n",
      "\n",
      "Epoch 00007: loss improved from 2.59576 to 2.55044, saving model to weights-improvement-07-2.5504-bigger.hdf5\n",
      "Epoch 8/50\n",
      "144309/144309 [==============================] - 85s 589us/step - loss: 2.5110\n",
      "\n",
      "Epoch 00008: loss improved from 2.55044 to 2.51096, saving model to weights-improvement-08-2.5110-bigger.hdf5\n",
      "Epoch 9/50\n",
      "144309/144309 [==============================] - 82s 571us/step - loss: 2.4779\n",
      "\n",
      "Epoch 00009: loss improved from 2.51096 to 2.47790, saving model to weights-improvement-09-2.4779-bigger.hdf5\n",
      "Epoch 10/50\n",
      "144309/144309 [==============================] - 83s 578us/step - loss: 2.4437\n",
      "\n",
      "Epoch 00010: loss improved from 2.47790 to 2.44375, saving model to weights-improvement-10-2.4437-bigger.hdf5\n",
      "Epoch 11/50\n",
      "144309/144309 [==============================] - 82s 568us/step - loss: 2.4164\n",
      "\n",
      "Epoch 00011: loss improved from 2.44375 to 2.41639, saving model to weights-improvement-11-2.4164-bigger.hdf5\n",
      "Epoch 12/50\n",
      "144309/144309 [==============================] - 82s 570us/step - loss: 2.3912\n",
      "\n",
      "Epoch 00012: loss improved from 2.41639 to 2.39115, saving model to weights-improvement-12-2.3912-bigger.hdf5\n",
      "Epoch 13/50\n",
      "144309/144309 [==============================] - 82s 569us/step - loss: 2.3650\n",
      "\n",
      "Epoch 00013: loss improved from 2.39115 to 2.36505, saving model to weights-improvement-13-2.3650-bigger.hdf5\n",
      "Epoch 14/50\n",
      "144309/144309 [==============================] - 81s 562us/step - loss: 2.3417\n",
      "\n",
      "Epoch 00014: loss improved from 2.36505 to 2.34166, saving model to weights-improvement-14-2.3417-bigger.hdf5\n",
      "Epoch 15/50\n",
      "144309/144309 [==============================] - 82s 566us/step - loss: 2.3216\n",
      "\n",
      "Epoch 00015: loss improved from 2.34166 to 2.32165, saving model to weights-improvement-15-2.3216-bigger.hdf5\n",
      "Epoch 16/50\n",
      "144309/144309 [==============================] - 83s 572us/step - loss: 2.3019\n",
      "\n",
      "Epoch 00016: loss improved from 2.32165 to 2.30186, saving model to weights-improvement-16-2.3019-bigger.hdf5\n",
      "Epoch 17/50\n",
      "144309/144309 [==============================] - 84s 583us/step - loss: 2.2809\n",
      "\n",
      "Epoch 00017: loss improved from 2.30186 to 2.28092, saving model to weights-improvement-17-2.2809-bigger.hdf5\n",
      "Epoch 18/50\n",
      "144309/144309 [==============================] - 84s 580us/step - loss: 2.2634\n",
      "\n",
      "Epoch 00018: loss improved from 2.28092 to 2.26340, saving model to weights-improvement-18-2.2634-bigger.hdf5\n",
      "Epoch 19/50\n",
      "144309/144309 [==============================] - 85s 587us/step - loss: 2.2463\n",
      "\n",
      "Epoch 00019: loss improved from 2.26340 to 2.24629, saving model to weights-improvement-19-2.2463-bigger.hdf5\n",
      "Epoch 20/50\n",
      "144309/144309 [==============================] - 84s 586us/step - loss: 2.2349\n",
      "\n",
      "Epoch 00020: loss improved from 2.24629 to 2.23491, saving model to weights-improvement-20-2.2349-bigger.hdf5\n",
      "Epoch 21/50\n",
      "144309/144309 [==============================] - 83s 573us/step - loss: 2.2189\n",
      "\n",
      "Epoch 00021: loss improved from 2.23491 to 2.21888, saving model to weights-improvement-21-2.2189-bigger.hdf5\n",
      "Epoch 22/50\n",
      "144309/144309 [==============================] - 83s 578us/step - loss: 2.2048\n",
      "\n",
      "Epoch 00022: loss improved from 2.21888 to 2.20481, saving model to weights-improvement-22-2.2048-bigger.hdf5\n",
      "Epoch 23/50\n",
      "144309/144309 [==============================] - 83s 578us/step - loss: 2.1960\n",
      "\n",
      "Epoch 00023: loss improved from 2.20481 to 2.19602, saving model to weights-improvement-23-2.1960-bigger.hdf5\n",
      "Epoch 24/50\n",
      "144309/144309 [==============================] - 84s 579us/step - loss: 2.1853\n",
      "\n",
      "Epoch 00024: loss improved from 2.19602 to 2.18527, saving model to weights-improvement-24-2.1853-bigger.hdf5\n",
      "Epoch 25/50\n",
      "144309/144309 [==============================] - 82s 570us/step - loss: 2.1770\n",
      "\n",
      "Epoch 00025: loss improved from 2.18527 to 2.17703, saving model to weights-improvement-25-2.1770-bigger.hdf5\n",
      "Epoch 26/50\n",
      "144309/144309 [==============================] - 82s 566us/step - loss: 2.1614\n",
      "\n",
      "Epoch 00026: loss improved from 2.17703 to 2.16139, saving model to weights-improvement-26-2.1614-bigger.hdf5\n",
      "Epoch 27/50\n",
      "144309/144309 [==============================] - 83s 577us/step - loss: 2.1498\n",
      "\n",
      "Epoch 00027: loss improved from 2.16139 to 2.14977, saving model to weights-improvement-27-2.1498-bigger.hdf5\n",
      "Epoch 28/50\n",
      "144309/144309 [==============================] - 83s 574us/step - loss: 2.1423\n",
      "\n",
      "Epoch 00028: loss improved from 2.14977 to 2.14231, saving model to weights-improvement-28-2.1423-bigger.hdf5\n",
      "Epoch 29/50\n",
      "144309/144309 [==============================] - 83s 573us/step - loss: 2.1321\n",
      "\n",
      "Epoch 00029: loss improved from 2.14231 to 2.13214, saving model to weights-improvement-29-2.1321-bigger.hdf5\n",
      "Epoch 30/50\n",
      "144309/144309 [==============================] - 84s 581us/step - loss: 2.1243\n",
      "\n",
      "Epoch 00030: loss improved from 2.13214 to 2.12426, saving model to weights-improvement-30-2.1243-bigger.hdf5\n",
      "Epoch 31/50\n",
      "144309/144309 [==============================] - 83s 578us/step - loss: 2.1184\n",
      "\n",
      "Epoch 00031: loss improved from 2.12426 to 2.11838, saving model to weights-improvement-31-2.1184-bigger.hdf5\n",
      "Epoch 32/50\n",
      "144309/144309 [==============================] - 83s 578us/step - loss: 2.1081\n",
      "\n",
      "Epoch 00032: loss improved from 2.11838 to 2.10809, saving model to weights-improvement-32-2.1081-bigger.hdf5\n",
      "Epoch 33/50\n",
      "144309/144309 [==============================] - 83s 574us/step - loss: 2.1041\n",
      "\n",
      "Epoch 00033: loss improved from 2.10809 to 2.10408, saving model to weights-improvement-33-2.1041-bigger.hdf5\n",
      "Epoch 34/50\n",
      "144309/144309 [==============================] - 83s 575us/step - loss: 2.0944\n",
      "\n",
      "Epoch 00034: loss improved from 2.10408 to 2.09435, saving model to weights-improvement-34-2.0944-bigger.hdf5\n",
      "Epoch 35/50\n",
      "144309/144309 [==============================] - 84s 579us/step - loss: 2.0891\n",
      "\n",
      "Epoch 00035: loss improved from 2.09435 to 2.08908, saving model to weights-improvement-35-2.0891-bigger.hdf5\n",
      "Epoch 36/50\n",
      "144309/144309 [==============================] - 83s 575us/step - loss: 2.0777\n",
      "\n",
      "Epoch 00036: loss improved from 2.08908 to 2.07772, saving model to weights-improvement-36-2.0777-bigger.hdf5\n",
      "Epoch 37/50\n",
      "144309/144309 [==============================] - 84s 582us/step - loss: 2.0729\n",
      "\n",
      "Epoch 00037: loss improved from 2.07772 to 2.07289, saving model to weights-improvement-37-2.0729-bigger.hdf5\n",
      "Epoch 38/50\n",
      "144309/144309 [==============================] - 84s 581us/step - loss: 2.0633\n",
      "\n",
      "Epoch 00038: loss improved from 2.07289 to 2.06334, saving model to weights-improvement-38-2.0633-bigger.hdf5\n",
      "Epoch 39/50\n",
      "144309/144309 [==============================] - 84s 584us/step - loss: 2.0594\n",
      "\n",
      "Epoch 00039: loss improved from 2.06334 to 2.05936, saving model to weights-improvement-39-2.0594-bigger.hdf5\n",
      "Epoch 40/50\n",
      "144309/144309 [==============================] - 82s 568us/step - loss: 2.0549\n",
      "\n",
      "Epoch 00040: loss improved from 2.05936 to 2.05489, saving model to weights-improvement-40-2.0549-bigger.hdf5\n",
      "Epoch 41/50\n",
      "144309/144309 [==============================] - 83s 575us/step - loss: 2.0519\n",
      "\n",
      "Epoch 00041: loss improved from 2.05489 to 2.05189, saving model to weights-improvement-41-2.0519-bigger.hdf5\n",
      "Epoch 42/50\n",
      "144309/144309 [==============================] - 83s 572us/step - loss: 2.0453\n",
      "\n",
      "Epoch 00042: loss improved from 2.05189 to 2.04530, saving model to weights-improvement-42-2.0453-bigger.hdf5\n",
      "Epoch 43/50\n",
      "144309/144309 [==============================] - 81s 561us/step - loss: 2.0404\n",
      "\n",
      "Epoch 00043: loss improved from 2.04530 to 2.04037, saving model to weights-improvement-43-2.0404-bigger.hdf5\n",
      "Epoch 44/50\n",
      "144309/144309 [==============================] - 82s 566us/step - loss: 2.0363\n",
      "\n",
      "Epoch 00044: loss improved from 2.04037 to 2.03632, saving model to weights-improvement-44-2.0363-bigger.hdf5\n",
      "Epoch 45/50\n",
      "144309/144309 [==============================] - 82s 571us/step - loss: 2.0336\n",
      "\n",
      "Epoch 00045: loss improved from 2.03632 to 2.03356, saving model to weights-improvement-45-2.0336-bigger.hdf5\n",
      "Epoch 46/50\n",
      "144309/144309 [==============================] - 83s 573us/step - loss: 2.0419\n",
      "\n",
      "Epoch 00046: loss did not improve from 2.03356\n",
      "Epoch 47/50\n",
      "144309/144309 [==============================] - 82s 568us/step - loss: 2.0506\n",
      "\n",
      "Epoch 00047: loss did not improve from 2.03356\n",
      "Epoch 48/50\n",
      "144309/144309 [==============================] - 82s 569us/step - loss: 2.0364\n",
      "\n",
      "Epoch 00048: loss did not improve from 2.03356\n",
      "Epoch 49/50\n",
      "144309/144309 [==============================] - 81s 562us/step - loss: 2.0304\n",
      "\n",
      "Epoch 00049: loss improved from 2.03356 to 2.03044, saving model to weights-improvement-49-2.0304-bigger.hdf5\n",
      "Epoch 50/50\n",
      "144309/144309 [==============================] - 81s 561us/step - loss: 2.0083\n",
      "\n",
      "Epoch 00050: loss improved from 2.03044 to 2.00827, saving model to weights-improvement-50-2.0083-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f2df9f79dd8>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=512, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text using saved files as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDlCYYrwRXAE"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model(\"weights-improvement-50-2.0083-bigger.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "HTdPo2ZJJsEz",
    "outputId": "c753a271-2ba9-4534-ca9e-a65b74853ad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" nearly as large as himself, and this he handed over to the other,\n",
      "saying, in a solemn tone, 'for the \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll ask the model to predict what comes next based off of the random seed, convert the output numbers to characters and then append it to the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "wZTB0VWhJsE2",
    "outputId": "ed86c04c-b11b-4b30-c71e-267cf5b567a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mock turtle so the cook the sabbit to she courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate  1000 characters\n",
    "import sys\n",
    "for i in range(1000):\n",
    "    # Get the pattern and predict the new char\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model1.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    \n",
    "    \n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    \n",
    "    # Update the patten by eliminating the first char and adding\n",
    "    # the predicted char at the end of the the pattern to generate new char\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text that was generated doesn't make any sense, and it seems to start simply repeating patterns after a little bit. However, the longer you train the network the better the text that is generated will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uu60OP9qetZJ"
   },
   "source": [
    "## Generate Text using saved files as model's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weight to the model and compile it\n",
    "filename = \"weights-improvement-50-2.0083-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" tle. 'seals, turtles, salmon, and so on;\n",
      "then, when you've cleared all the jelly-fish out of the way \"\n",
      " in the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the courd to the cour\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
