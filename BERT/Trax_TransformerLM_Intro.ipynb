{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trax TransformerLM Intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE38oeYU9fgv",
        "colab_type": "text"
      },
      "source": [
        "# TransformerLM Quick Start and Guide \n",
        "Language models are machine learning models that power some of the most impressive applications involving text and language (e.g. machine translation, sentiment analysis, chatbots, summarization). At the time of this writing, some of the largest ML models in existence are language models. They are also based on the [transformer](https://arxiv.org/abs/1706.03762) architecture. The transformer language model (TransformerLM) is a simpler [variation](https://arxiv.org/pdf/1801.10198.pdf) of the original transformer architecture and is useful for plenty of tasks.\n",
        "\n",
        "<img style=\"max-width:350px;\" src=\"https://storage.googleapis.com/ml-intro/t/transformerLM-1.png\" />\n",
        "\n",
        "The [Trax](https://trax-ml.readthedocs.io/en/latest/) implementation of TransformerLM focuses on clear code and speed.  It runs without any changes on CPUs, GPUs and TPUs.\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Use a pre-trained TransformerLM\n",
        "2. Train a TransformerLM model\n",
        "3. Looking inside the Trax TransformerLM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWKJgBwkeTax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "53e1bfe0-64d5-48be-a17d-955a9b2a9cf8"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "! pip install -q -U trax\n",
        "import trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 368kB 3.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 11.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 31.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 307kB 57.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5MB 20.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 778kB 60.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 55.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 58.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 57.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 58.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 17.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 55.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 55.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 49.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 57.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25h  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: t5 0.6.3 has requirement tensorflow-text<2.3, but you'll have tensorflow-text 2.3.0 which is incompatible.\u001b[0m\n",
            "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWxTtb4snypO",
        "colab_type": "text"
      },
      "source": [
        "## Using a pre-trained TransformerLM\n",
        "\n",
        "The following cell loads a pre-trained TransformerLM that sorts a list of four integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL3rakwb05cL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2f791cf5-41f0-46d8-f971-4e3f00edf530"
      },
      "source": [
        "# Create a Transformer model.\n",
        "# Have to use the same configuration of the pre-trained model we'll load next\n",
        "model = trax.models.TransformerLM(  \n",
        "    d_model=32, d_ff=128, n_layers=2, \n",
        "    vocab_size=32, mode='predict')\n",
        "\n",
        "# Initialize using pre-trained weights.\n",
        "model.init_from_file('gs://ml-intro/models/sort-transformer.pkl.gz',\n",
        "                     weights_only=True, \n",
        "                     input_signature=trax.shapes.ShapeDtype((1,1), dtype=np.int32))\n",
        "\n",
        "# Input sequence\n",
        "# The 0s indicate the beginning and end of the input sequence\n",
        "input = [0, 3, 14, 15, 9, 0]\n",
        "\n",
        "\n",
        "# Run the model\n",
        "output = trax.supervised.decoding.autoregressive_sample(\n",
        "    model, np.array([input]), temperature=0.0, max_length=4)\n",
        "\n",
        "# Show us the output\n",
        "output"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
            "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  9, 14, 15]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5rohWvyoOaf",
        "colab_type": "text"
      },
      "source": [
        "This is a trivial example to get you started and put a toy transformer into your hands. Language models get their name from their ability to assign probabilities to sequences of words. This property makes them useful for generating text (and other types of sequences) by choosing the highest probability item as the next item in the sequence -- exactly like the next-word suggestion feature of your smartphone keyboard.\n",
        "\n",
        "In Trax, TransformerLM is a series of [Layers]() combined using the [Serial]() combinator. A high level view of the TransformerLM we've declared above can look like this:\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/ml-intro/t/transformerLM-layers-1.png\" />\n",
        "\n",
        "The model has two decoder layers because we set `n_layers` to 2. TransformerLM makes predictions by being fed one token at a time, with output tokens typically fedback as inputs (that's the `autoregressive` part of the `autoregressive_sample` method we used to generate the output from the model). \n",
        "\n",
        "If we're to think of a simple model that takes the sequence `1, 2` and returns `3, 4`, this is how that process would look like:\n",
        "<img src=\"https://storage.googleapis.com/ml-intro/t/transformerLM%20input-output.gif\" />\n",
        "\n",
        "## Train a TransformerLM Model\n",
        "\n",
        "Let's train a TransformerLM model. We'll train this one to reverse a list of integers. This is another toy task that we can train a small transformer to do. But using the concepts we'll go over, you'll be able to train proper language models on larger dataset.\n",
        "\n",
        "**Example**: This model is to take a sequence like `[1, 2, 3, 4]` and return `[4, 3, 2, 1]`.\n",
        "\n",
        "1. Create the Model\n",
        "1. Prepare the Dataset\n",
        "1. Train the model using `Trainer`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vra3JRlJtilo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Create the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vz4xGIRur_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Transformer model.\n",
        "def tiny_transformer_lm(mode='train'):\n",
        "  return trax.models.TransformerLM(  \n",
        "          d_model=32, d_ff=128, n_layers=2, \n",
        "          vocab_size=32, mode=mode)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtHa8mOmb-NZ",
        "colab_type": "text"
      },
      "source": [
        "Refer to [TransferLM in the API reference](https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.transformer.TransformerLM) to understand each of its parameters and their default values. We have chosen to create a small model using these values for `d_model`, `d_ff`, and `n_layers` to be able to train the model more quickly on this simple task.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/ml-intro/t/untrained-transformer.png\" />\n",
        "\n",
        "### Prepare the Dataset\n",
        "\n",
        "Trax models are trained on streams of data represented as python iterators. [`trax.data`](https://trax-ml.readthedocs.io/en/latest/trax.data.html) gives you the tools to construct your datapipeline. Trax also gives you readily available access to [TensorFlow Datasets](https://www.tensorflow.org/datasets).\n",
        "\n",
        "For this simple task, we will create a python generator. Every time we invoke it, it returns a batch of training examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_JoHNp_1IUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reverse_ints_task(batch_size, length=4):\n",
        "  while True:\n",
        "    random_ints = m = np.random.randint(1, 31, (batch_size,length))\n",
        "    source = random_ints\n",
        "\n",
        "    target = np.flip(source, 1)\n",
        "\n",
        "    zero = np.zeros([batch_size, 1], np.int32)\n",
        "    x = np.concatenate([zero, source, zero, target], axis=1)\n",
        "\n",
        "    loss_weights = np.concatenate([np.zeros((batch_size, length+2)),\n",
        "                                    np.ones((batch_size, length))], axis=1)\n",
        "    yield (x, x, loss_weights)  # Here inputs and targets are the same.\n",
        "\n",
        "reverse_ints_inputs = trax.data.inputs.Inputs(lambda _: reverse_ints_task(16))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtzNKw0Flf-v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "This function prepares a dataset and returns one batch at a time. If we ask for a batch size of 8, for example, it returns the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgPlRp1AjJ2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "cd393c20-5c73-4d5c-a2f9-cfde4387362c"
      },
      "source": [
        "a = reverse_ints_task(8)\n",
        "sequence_batch, _ , masks = next(a)\n",
        "sequence_batch"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  9, 18,  2, 14,  0, 14,  2, 18,  9],\n",
              "       [ 0, 28, 18,  5,  3,  0,  3,  5, 18, 28],\n",
              "       [ 0, 20, 20, 13, 21,  0, 21, 13, 20, 20],\n",
              "       [ 0, 16, 14, 26, 12,  0, 12, 26, 14, 16],\n",
              "       [ 0, 16,  5, 25,  5,  0,  5, 25,  5, 16],\n",
              "       [ 0,  7, 14, 17, 12,  0, 12, 17, 14,  7],\n",
              "       [ 0, 20, 15, 10, 10,  0, 10, 10, 15, 20],\n",
              "       [ 0, 29, 18, 19, 28,  0, 28, 19, 18, 29]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uae3WdEA9etU",
        "colab_type": "text"
      },
      "source": [
        "You can see that each example starts with 0, then a list of integers, then another 0, then the reverse of the list of integers. The function will give us as many examples and batches as we request.\n",
        "\n",
        "In addition to the example, the generator returns a mask vector. During the training process, the model is challeneged to predict the tokens hidden by the mask (which have a value of 1 associated with that position. So for example, if the first element in the batch is the following vector:\n",
        "\n",
        "<table><tr>\n",
        "<td><strong>0</strong></td><td>5</td><td>6</td><td>7</td><td>8</td><td><strong>0</strong></td><td>8</td><td>7</td><td>6</td><td>5</td>\n",
        "</tr></table> \n",
        "\n",
        "And the associated mask vector for this example is:\n",
        "<table><tr>\n",
        "<td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td>\n",
        "</tr></table> \n",
        "\n",
        "Then the model will only be presented with the following prefix items, and it has to predict the rest:\n",
        "<table><tr>\n",
        "<td><strong>0</strong></td><td>5</td><td>6</td><td>7</td><td>8</td><td><strong>0</strong></td><td>_</td><td>_</td><td>_ </td><td>_</td>\n",
        "</tr></table> \n",
        "\n",
        "It's important here to note that while `5, 6, 7, 8` constitute the input sequence, the **zeros** serve a different purpose. We are using them as special tokens to delimit where the source sequence begins and ends. \n",
        "\n",
        "With this, we now have a method that streams the dataset in addition to the method that creates the model.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/ml-intro/t/untrained-transformer-and-dataset.png\" />\n",
        "\n",
        "\n",
        "### Train the model using `Trainer`\n",
        "\n",
        "Trax's [Trainer](https://trax-ml.readthedocs.io/en/latest/trax.html#module-trax.trainer) takes care of the training process. We hand it the model, the method streaming the dataset, and other training parameters. We then start the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itAKBkN81H1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "4ab2bf99-5411-40cb-e247-71cbd2d72ec4"
      },
      "source": [
        "output_dir = os.path.expanduser('~/train_dir/')\n",
        "!rm -f ~/train_dir/model.pkl.gz  # Remove old model.\n",
        "\n",
        "# Train tiny model with Trainer.\n",
        "trainer = trax.supervised.Trainer(\n",
        "    model=tiny_transformer_lm,\n",
        "    loss_fn=trax.layers.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adafactor,  # Change optimizer params here.\n",
        "    lr_schedule=trax.lr.constant(0.001),  # Change lr schedule here.\n",
        "    inputs=reverse_ints_inputs,\n",
        "    output_dir=output_dir)\n",
        "\n",
        "# Train for 3 epochs each consisting of 500 train batches, eval on 2 batches.\n",
        "n_epochs  = 3\n",
        "train_steps = 800\n",
        "eval_steps = 2\n",
        "for _ in range(n_epochs):\n",
        "  trainer.train_epoch(train_steps, eval_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step    800: Ran 800 train steps in 56.55 secs\n",
            "Step    800: Evaluation\n",
            "Step    800: train                   accuracy |  0.32812500\n",
            "Step    800: train                       loss |  2.61360502\n",
            "Step    800: train         neg_log_perplexity | -2.61360502\n",
            "Step    800: train          sequence_accuracy |  0.00000000\n",
            "Step    800: train weights_per_batch_per_core |  64.00000000\n",
            "Step    800: eval                    accuracy |  0.25000000\n",
            "Step    800: eval                        loss |  2.71211648\n",
            "Step    800: eval          neg_log_perplexity | -2.71211648\n",
            "Step    800: eval           sequence_accuracy |  0.00000000\n",
            "Step    800: eval  weights_per_batch_per_core |  64.00000000\n",
            "Step    800: Finished evaluation\n",
            "\n",
            "Step   1600: Ran 800 train steps in 6.29 secs\n",
            "Step   1600: Evaluation\n",
            "Step   1600: train                   accuracy |  0.74218750\n",
            "Step   1600: train                       loss |  1.02415180\n",
            "Step   1600: train         neg_log_perplexity | -1.02415180\n",
            "Step   1600: train          sequence_accuracy |  0.18750000\n",
            "Step   1600: train weights_per_batch_per_core |  64.00000000\n",
            "Step   1600: eval                    accuracy |  0.66406250\n",
            "Step   1600: eval                        loss |  1.16481757\n",
            "Step   1600: eval          neg_log_perplexity | -1.16481757\n",
            "Step   1600: eval           sequence_accuracy |  0.12500000\n",
            "Step   1600: eval  weights_per_batch_per_core |  64.00000000\n",
            "Step   1600: Finished evaluation\n",
            "\n",
            "Step   2400: Ran 800 train steps in 6.03 secs\n",
            "Step   2400: Evaluation\n",
            "Step   2400: train                   accuracy |  1.00000000\n",
            "Step   2400: train                       loss |  0.03707326\n",
            "Step   2400: train         neg_log_perplexity | -0.03707326\n",
            "Step   2400: train          sequence_accuracy |  1.00000000\n",
            "Step   2400: train weights_per_batch_per_core |  64.00000000\n",
            "Step   2400: eval                    accuracy |  1.00000000\n",
            "Step   2400: eval                        loss |  0.04914701\n",
            "Step   2400: eval          neg_log_perplexity | -0.04914701\n",
            "Step   2400: eval           sequence_accuracy |  1.00000000\n",
            "Step   2400: eval  weights_per_batch_per_core |  64.00000000\n",
            "Step   2400: Finished evaluation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umb-MvIJme65",
        "colab_type": "text"
      },
      "source": [
        "The Trainer is the third key component in this process that helps us arrive at the trained model.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/ml-intro/t/transformerLM-training.png\" />\n",
        "\n",
        "### Make predictions\n",
        "\n",
        "Let's take our newly minted model for a ride. To do that, we load it up, and use the handy `autoregressive_sample` method to feed it our input sequence and return the output sequence. These components now look like this:\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/ml-intro/t/transformerLM-sampling-prediction.png\" />\n",
        "\n",
        "And this is the code to do just that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psqZzQELeMxE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "430ecd7c-4e02-43be-d82e-be4ed08487cc"
      },
      "source": [
        "\n",
        "input = np.array([[0, 4, 6, 8, 10, 0]])\n",
        "\n",
        "# Initialize model for inference.\n",
        "predict_model = tiny_transformer_lm(mode='predict')\n",
        "predict_signature = trax.shapes.ShapeDtype((1,1), dtype=np.int32)\n",
        "predict_model.init_from_file(os.path.join(output_dir, \"model.pkl.gz\"),\n",
        "                             weights_only=True, input_signature=predict_signature)\n",
        "\n",
        "# Run the model\n",
        "output = trax.supervised.decoding.autoregressive_sample(\n",
        "    predict_model, input, temperature=0.0, max_length=4)\n",
        "\n",
        "# Print the contents of output\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10,  8,  6,  4]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERq8BDIvD8GP",
        "colab_type": "text"
      },
      "source": [
        "## TODO: Looking inside the Trax TransformerLM\n",
        "Visualize tl.Serial, Layers, and the parameters to initialize the model\n",
        "\n",
        "Work in progress visualization:\n",
        "<img src=\"https://storage.googleapis.com/ml-intro/t/trax-layers-serial-draft.png\" />\n",
        "\n",
        "## TODO: Training a proper language model\n",
        "Train a language model to generate text. Character level is perhaps a more gentle learning slope, but I see the merit in also going directly into a word-based model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Iw5EG9Ry-ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}